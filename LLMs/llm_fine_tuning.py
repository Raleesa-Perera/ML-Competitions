# -*- coding: utf-8 -*-
"""LLM Fine tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s5kDRNlUezs9by1dRUD-cZ-OQU4y91S3
"""

# Install necessary libraries
!pip install -q accelerate bitsandbytes datasets transformers

!pip install -U protobuf==5.29.1 fsspec==2025.3.2

!pip install -q unsloth unsloth_zoo

from unsloth import FastLanguageModel
import torch

# Load tokenizer + model in 4-bit mode
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "Qwen/Qwen2.5-3B-Instruct",
    max_seq_length = 2048,
    load_in_4bit = True,
    dtype = torch.float16,
    device_map = "auto"
)

from datasets import Dataset

# A very small QA dataset for testing
data = [
    {
        "instruction": "What is LoRA in machine learning?",
        "input": "LoRA stands for Low-Rank Adaptation...",
        "output": "LoRA is a technique that adds low-rank matrices to fine-tune large models efficiently."
    },
    {
        "instruction": "What is QLoRA?",
        "input": "QLoRA is a recent method used to fine-tune LLMs in 4-bit precision...",
        "output": "QLoRA allows full fine-tuning of large models using low memory by quantizing the model."
    }
]

dataset = Dataset.from_list(data)
dataset = dataset.train_test_split(test_size=0.2)

import unsloth
from unsloth import FastLanguageModel, UnslothTrainer
from transformers import TrainingArguments

# Apply QLoRA to the model without `task_type`
model = FastLanguageModel.get_peft_model(
    model,
    r = 8,  # Rank of LoRA
    lora_alpha = 16,  # Alpha scaling factor for LoRA
    lora_dropout = 0.05,  # Dropout for LoRA layers
    bias = "none",  # Don't fine-tune bias layers
)

# Define training config
training_args = TrainingArguments(
    output_dir = "qlora-qwen2.5",  # Output directory for model checkpoints
    per_device_train_batch_size = 1,  # Batch size per device (GPU)
    gradient_accumulation_steps = 2,  # Steps to accumulate gradients
    num_train_epochs = 1,  # Number of training epochs
    learning_rate = 2e-4,  # Learning rate for training
    fp16 = True,  # Enable mixed precision training for faster performance
    logging_steps = 1,  # Log training progress every step
    save_strategy = "no",  # Don't save intermediate checkpoints
)

# Use Unslothâ€™s Trainer (UnslothTrainer) for fine-tuning
trainer = UnslothTrainer(
    model = model,
    train_dataset = dataset["train"],  # Your training dataset
    eval_dataset = dataset["test"],    # Your test dataset
    tokenizer = tokenizer,  # Tokenizer for text processing
    args = training_args,  # Training configurations
    dataset_text_field = "output",  # Name of the text field in dataset
)

trainer.train()

!pip install -U transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "Qwen/Qwen2.5-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

prompt = "### Instruction:\nWhat is LoRA?\n\n### Response:"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=200)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))